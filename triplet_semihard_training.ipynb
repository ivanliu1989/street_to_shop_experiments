{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import multiprocessing as mp\n",
    "import math\n",
    "import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "import json\n",
    "import PIL\n",
    "from functools import partial, update_wrapper\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 299     # input image size for network\n",
    "margin = 0.3       # margin for triplet loss\n",
    "batch_size = 42    # size of mini-batch\n",
    "num_triplets = 700 \n",
    "valid_frequency = 100\n",
    "num_epoch = 3600 \n",
    "log_dir = './logs/triplet_semihard_v3'\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "recall_log_file = './logs/recall_triplet_semihard_v3.json'\n",
    "recall_values = [1, 3, 5, 10, 25, 50, 100]\n",
    "image_dir = '../../../DeepFashion2_Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to freeze some portion of a function's arguments\n",
    "def wrapped_partial(func, *args, **kwargs):\n",
    "    partial_func = partial(func, *args, **kwargs)\n",
    "    update_wrapper(partial_func, func)\n",
    "    return partial_func\n",
    "\n",
    "# calculate recall score\n",
    "def recall(y_true, y_pred):\n",
    "    return min(len(set(y_true) & set(y_pred)), 1)\n",
    "\n",
    "# margin triplet loss\n",
    "def margin_triplet_loss(y_true, y_pred, margin, batch_size):\n",
    "    out_a = tf.gather(y_pred, tf.range(0, batch_size, 3))\n",
    "    out_p = tf.gather(y_pred, tf.range(1, batch_size, 3))\n",
    "    out_n = tf.gather(y_pred, tf.range(2, batch_size, 3))\n",
    "    \n",
    "    loss = K.maximum(margin\n",
    "                 + K.sum(K.square(out_a-out_p), axis=1)\n",
    "                 - K.sum(K.square(out_a-out_n), axis=1),\n",
    "                 0.0)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "#     no_top_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg')\n",
    "\n",
    "#     x = no_top_model.output\n",
    "#     x = Dense(512, activation='elu', name='fc1')(x)\n",
    "#     x = Dense(128, name='fc2')(x)\n",
    "#     x = Lambda(lambda x: K.l2_normalize(x, axis=1), name='l2_norm')(x)\n",
    "#     return Model(no_top_model.inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model():\n",
    "    model = tf.keras.models.load_model('./models/triplet_semihard_final.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x1a39e526d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_trained_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=wrapped_partial(margin_triplet_loss, margin=margin, batch_size=batch_size), optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x1a39e526d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used mongodb to store dataset. Document structure:\n",
    "- type: define split on train, validation and test sets, possible values - 'train', 'val', 'test'\n",
    "- seller_img: list of IDs of seller's images\n",
    "- user_img: list of IDs of user's images\n",
    "\n",
    "Example:\n",
    "```javascript\n",
    "{\n",
    "    'type': 'test',\n",
    "    'seller_img': ['HTB1s7ZiLFXXXXatXFXXq6xXFXXXu', \n",
    "                'HTB1pGaAKXXXXXczXXXXq6xXFXXXN'],\n",
    "    'user_img': ['UTB8KtbUXXPJXKJkSahVq6xyzFXaE',\n",
    "                'UTB8OeDUXgnJXKJkSaelq6xUzXXag',\n",
    "                'UTB8h7HUXnzIXKJkSafVq6yWgXXap',\n",
    "                'UTB8auL6XevJXKJkSajhq6A7aFXaa',\n",
    "                'UTB8rrevXevJXKJkSajhq6A7aFXa5',\n",
    "                'UTB8WUCuXXPJXKJkSahVq6xyzFXa6',\n",
    "                'UTB8MHmvXXfJXKJkSamHq6zLyVXa1']\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>bounding_box</th>\n",
       "      <th>style</th>\n",
       "      <th>pair_id</th>\n",
       "      <th>source</th>\n",
       "      <th>val</th>\n",
       "      <th>pair_unique_id</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364671</th>\n",
       "      <td>022559.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>short sleeve top</td>\n",
       "      <td>[0, 0, 433, 505]</td>\n",
       "      <td>1</td>\n",
       "      <td>1318</td>\n",
       "      <td>shop</td>\n",
       "      <td>validation</td>\n",
       "      <td>1318-1-1</td>\n",
       "      <td>validation/image/022559.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364672</th>\n",
       "      <td>022559.jpg</td>\n",
       "      <td>8</td>\n",
       "      <td>trousers</td>\n",
       "      <td>[145, 373, 466, 701]</td>\n",
       "      <td>0</td>\n",
       "      <td>1318</td>\n",
       "      <td>shop</td>\n",
       "      <td>validation</td>\n",
       "      <td>1318-8-0</td>\n",
       "      <td>validation/image/022559.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364673</th>\n",
       "      <td>016905.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>short sleeve top</td>\n",
       "      <td>[304, 3, 586, 324]</td>\n",
       "      <td>2</td>\n",
       "      <td>670</td>\n",
       "      <td>shop</td>\n",
       "      <td>validation</td>\n",
       "      <td>670-1-2</td>\n",
       "      <td>validation/image/016905.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364674</th>\n",
       "      <td>016905.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>shorts</td>\n",
       "      <td>[341, 285, 556, 514]</td>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "      <td>shop</td>\n",
       "      <td>validation</td>\n",
       "      <td>670-7-0</td>\n",
       "      <td>validation/image/016905.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364675</th>\n",
       "      <td>017617.jpg</td>\n",
       "      <td>12</td>\n",
       "      <td>vest dress</td>\n",
       "      <td>[275, 227, 527, 823]</td>\n",
       "      <td>1</td>\n",
       "      <td>738</td>\n",
       "      <td>shop</td>\n",
       "      <td>validation</td>\n",
       "      <td>738-12-1</td>\n",
       "      <td>validation/image/017617.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  category_id     category_name          bounding_box  \\\n",
       "364671  022559.jpg            1  short sleeve top      [0, 0, 433, 505]   \n",
       "364672  022559.jpg            8          trousers  [145, 373, 466, 701]   \n",
       "364673  016905.jpg            1  short sleeve top    [304, 3, 586, 324]   \n",
       "364674  016905.jpg            7            shorts  [341, 285, 556, 514]   \n",
       "364675  017617.jpg           12        vest dress  [275, 227, 527, 823]   \n",
       "\n",
       "        style  pair_id source         val pair_unique_id  \\\n",
       "364671      1     1318   shop  validation       1318-1-1   \n",
       "364672      0     1318   shop  validation       1318-8-0   \n",
       "364673      2      670   shop  validation        670-1-2   \n",
       "364674      0      670   shop  validation        670-7-0   \n",
       "364675      1      738   shop  validation       738-12-1   \n",
       "\n",
       "                         image_path  \n",
       "364671  validation/image/022559.jpg  \n",
       "364672  validation/image/022559.jpg  \n",
       "364673  validation/image/016905.jpg  \n",
       "364674  validation/image/016905.jpg  \n",
       "364675  validation/image/017617.jpg  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dt_all = pd.read_csv('deepfashion_retreival_train_val.csv')\n",
    "dt_all['image_path'] = dt_all['val'].map(str) + '/image/' + dt_all['image_id'].map(str)\n",
    "dt_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_all.category_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seller_images = []\n",
    "train_user_images = []\n",
    "# 221563 rows\n",
    "categories = ['long sleeve top', 'short sleeve top',\n",
    "       'vest dress', 'vest', 'short sleeve dress',\n",
    "       'sling dress', 'long sleeve outwear', 'long sleeve dress', 'sling',\n",
    "       'short sleeve outwear']\n",
    "# 145420 rows\n",
    "# categories = ['shorts', 'skirt', 'trousers', 'sling']\n",
    "\n",
    "for pair_unique_id in dt_all[dt_all.category_name.isin(categories)].pair_unique_id.unique():\n",
    "    seller_imgs = dt_all[(dt_all.pair_unique_id == pair_unique_id) & (dt_all.source == 'shop')].image_path.values\n",
    "    user_imgs = dt_all[(dt_all.pair_unique_id == pair_unique_id) & (dt_all.source == 'user')].image_path.values\n",
    "    if len(seller_imgs)>0 and len(user_imgs)>0:\n",
    "        train_seller_images.append(seller_imgs)\n",
    "        train_user_images.append(user_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15597"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_user_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```javascript\n",
    "train_seller_images = [\n",
    "                        ['HTB1s7ZiLFXXXXatXFXXq6xXFXXXu', \n",
    "                        'HTB1pGaAKXXXXXczXXXXq6xXFXXXN'],\n",
    "                        ['HTB1s7ZiLFXXXXatXFXXq6xXFXXXu', \n",
    "                        'HTB1pGaAKXXXXXczXXXXq6xXFXXXN'],\n",
    "                        ...\n",
    "                      ]\n",
    "\n",
    "train_user_images = [\n",
    "                        ['UTB8KtbUXXPJXKJkSahVq6xyzFXaE',\n",
    "                        'UTB8OeDUXgnJXKJkSaelq6xUzXXag',\n",
    "                        'UTB8h7HUXnzIXKJkSafVq6yWgXXap',\n",
    "                        'UTB8auL6XevJXKJkSajhq6A7aFXaa',\n",
    "                        'UTB8rrevXevJXKJkSajhq6A7aFXa5',\n",
    "                        'UTB8WUCuXXPJXKJkSahVq6xyzFXa6',\n",
    "                        'UTB8MHmvXXfJXKJkSamHq6zLyVXa1'],\n",
    "                        ['UTB8KtbUXXPJXKJkSahVq6xyzFXaE',\n",
    "                        'UTB8OeDUXgnJXKJkSaelq6xUzXXag',\n",
    "                        'UTB8h7HUXnzIXKJkSafVq6yWgXXap',\n",
    "                        'UTB8WUCuXXPJXKJkSahVq6xyzFXa6',\n",
    "                        'UTB8MHmvXXfJXKJkSamHq6zLyVXa1'],\n",
    "                        ...\n",
    "                      ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_user_images[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform(train_user_images[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train/image/013850.jpg', 'train/image/015077.jpg',\n",
       "       'train/image/015078.jpg', 'train/image/025419.jpg',\n",
       "       'train/image/046108.jpg', 'train/image/046109.jpg',\n",
       "       'train/image/046110.jpg', 'train/image/046111.jpg',\n",
       "       'train/image/046112.jpg', 'train/image/046113.jpg',\n",
       "       'train/image/046114.jpg', 'train/image/046115.jpg',\n",
       "       'train/image/046117.jpg', 'train/image/046118.jpg',\n",
       "       'train/image/058095.jpg', 'train/image/092998.jpg',\n",
       "       'train/image/092999.jpg', 'train/image/093000.jpg',\n",
       "       'train/image/093002.jpg', 'train/image/093003.jpg',\n",
       "       'train/image/115610.jpg', 'train/image/115611.jpg',\n",
       "       'train/image/115612.jpg', 'train/image/115613.jpg',\n",
       "       'train/image/115614.jpg', 'train/image/115615.jpg',\n",
       "       'train/image/115616.jpg', 'train/image/115617.jpg',\n",
       "       'train/image/115618.jpg', 'train/image/115619.jpg',\n",
       "       'train/image/115620.jpg', 'train/image/115621.jpg',\n",
       "       'train/image/115622.jpg', 'train/image/115623.jpg',\n",
       "       'train/image/116934.jpg', 'train/image/143077.jpg',\n",
       "       'train/image/143078.jpg', 'train/image/143079.jpg',\n",
       "       'train/image/143080.jpg', 'train/image/143081.jpg',\n",
       "       'train/image/143082.jpg', 'train/image/143083.jpg',\n",
       "       'train/image/143084.jpg', 'train/image/143085.jpg',\n",
       "       'train/image/143086.jpg', 'train/image/143087.jpg',\n",
       "       'train/image/143088.jpg', 'train/image/146767.jpg',\n",
       "       'train/image/146768.jpg', 'train/image/146769.jpg',\n",
       "       'train/image/146770.jpg', 'train/image/146771.jpg',\n",
       "       'train/image/146772.jpg', 'train/image/146773.jpg',\n",
       "       'train/image/146774.jpg', 'train/image/146775.jpg',\n",
       "       'train/image/146777.jpg'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seller = mlb.fit_transform(train_seller_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_seller = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_user = mlb.fit_transform(train_user_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_user = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_by_class = np.asarray(X_train_user.sum(axis=1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```javascript\n",
    "X_train_seller = [[0,0,0,1], [1,0,0,0]...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_images = [(x['seller_img'], x['user_img']) for x in coll.find({'type': 'val'})]\n",
    "# val_image_to_class = {}\n",
    "# for i, item in enumerate(val_images):\n",
    "#     for x in item[0] + item[1]:\n",
    "#         val_image_to_class[x] = i\n",
    "\n",
    "# val_images_clean = [(x['seller_img_clean'], x['user_img_clean']) for x in coll.find({'type': 'val', 'clean': True})]\n",
    "# val_image_clean_to_class = {}\n",
    "# for i, item in enumerate(val_images_clean):\n",
    "#     for x in item[0] + item[1]:\n",
    "#         val_image_clean_to_class[x] = i\n",
    "\n",
    "# test_images = [(x['seller_img'], x['user_img']) for x in coll.find({'type': 'test'})]\n",
    "\n",
    "# seller_images = [x for item in coll.find({}) for x in item['seller_img']]\n",
    "# val_user_images = [x for item in val_images for x in item[1]]\n",
    "# val_user_clean_images = [x for item in val_images_clean for x in item[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = []\n",
    "dt_val = dt_all[dt_all.val = 'validation']\n",
    "\n",
    "for pair_unique_id in dt_val[dt_val.category_name.isin(categories)].pair_unique_id.unique():\n",
    "    seller_imgs = dt_val[(dt_val.pair_unique_id == pair_unique_id) & (dt_val.source == 'shop')].image_path.values\n",
    "    user_imgs = dt_val[(dt_val.pair_unique_id == pair_unique_id) & (dt_val.source == 'user')].image_path.values\n",
    "    if len(seller_imgs)>0 and len(user_imgs)>0:\n",
    "        val_images.append((seller_imgs, user_imgs))\n",
    "        \n",
    "val_image_to_class = {}\n",
    "for i, item in enumerate(val_images):\n",
    "    for x in item[0] + item[1]:\n",
    "        val_image_to_class[x] = i\n",
    "\n",
    "        \n",
    "seller_images = dt_all[dt_all.source == 'shop'].image_path.values\n",
    "val_user_images = [x for item in val_images for x in item[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image by id without augmentations\n",
    "def preprocess_image_worker(media_id):\n",
    "    img = Image.open((image_dir + media_id)).convert('RGB')\n",
    "    img = img.resize((img_size, img_size))\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# preprocess_image_worker('validation/image/010779.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image by id with augmentations\n",
    "def preprocess_image_worker_aug(media_id):\n",
    "    img = Image.open((image_dir + media_id)).convert('RGB')\n",
    "    img = img.crop((int(np.random.uniform(0, 0.05)*img.width), int(np.random.uniform(0, 0.05)*img.height),\n",
    "                  int(np.random.uniform(0.95, 1.)*img.width), int(np.random.uniform(0.95, 1.)*img.height)))\n",
    "    if np.random.randint(2) == 0:\n",
    "        img = img.transpose(np.random.choice([PIL.Image.FLIP_LEFT_RIGHT, PIL.Image.FLIP_TOP_BOTTOM, PIL.Image.ROTATE_90, PIL.Image.ROTATE_180, PIL.Image.ROTATE_270, PIL.Image.TRANSPOSE]))\n",
    "    img = img.resize((img_size, img_size))\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# preprocess_image_worker_aug('validation/image/010779.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, divides list images into mini-batches\n",
    "def batch_generator_predict(pool, batch_size, images):\n",
    "    i = 0\n",
    "    while True:\n",
    "        batch = images[i:i+batch_size]\n",
    "        i += batch_size\n",
    "        if len(batch) == 0:\n",
    "            yield np.zeros((0, img_size, img_size, 3))\n",
    "        else:\n",
    "            result = pool.map(preprocess_image_worker, batch)\n",
    "            X_batch = np.concatenate(result, axis=0)\n",
    "            yield X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-hard negative mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiHardNegativeSampler:\n",
    "    def __init__(self, pool, batch_size, num_samples):\n",
    "        self.pool = pool\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = num_samples\n",
    "        self.resample()\n",
    "        \n",
    "    # sample triplets with semi-hard negatives\n",
    "    def resample(self):        \n",
    "        sample_classes = np.random.choice(np.arange(X_train_user.shape[0]), p=num_images_by_class/num_images_by_class.sum(), size=self.num_samples)\n",
    "\n",
    "        sample_images = []\n",
    "        for i in sample_classes:\n",
    "            sample_images.append(train_images_user[np.random.choice(X_train_user[i].nonzero()[1], replace=False)])\n",
    "            sample_images.append(train_images_seller[np.random.choice(X_train_seller[i].nonzero()[1], replace=False)])\n",
    "        sample_images = np.array(sample_images)\n",
    "\n",
    "        pred_sample = model.predict_generator(batch_generator_predict(pool, 32, sample_images), math.ceil(len(sample_images)/32), \n",
    "                                              # max_q_size=1, \n",
    "                                              workers=1)\n",
    "\n",
    "        a = pred_sample[np.arange(0, len(pred_sample), 2)]\n",
    "        p = pred_sample[np.arange(1, len(pred_sample), 2)]\n",
    "        triplets = []\n",
    "        self.dists = []\n",
    "\n",
    "        for i in range(self.num_samples):\n",
    "            d = np.square(a[i] - p[i]).sum()\n",
    "            neg_sample_classes = (sample_classes != sample_classes[i]).nonzero()[0]\n",
    "\n",
    "            neg = p[neg_sample_classes]\n",
    "\n",
    "            neg_ids = sample_images.reshape((-1, 2))[neg_sample_classes, 1]\n",
    "\n",
    "            d_neg = np.square(neg - a[i]).sum(axis=1)\n",
    "\n",
    "            semihard = np.where(d_neg > d)[0]\n",
    "\n",
    "            if len(semihard) == 0:\n",
    "                n = np.argmax(d_neg)\n",
    "            else:\n",
    "                n = semihard[np.argmin(d_neg[semihard])]\n",
    "                \n",
    "            self.dists.append(d_neg[n]-d)\n",
    "\n",
    "            triplets.append(np.concatenate([sample_images.reshape((-1, 2))[i], np.array([neg_ids[n]])]))\n",
    "\n",
    "        self.triplets = np.array(triplets)\n",
    "        \n",
    "    # data generator for triplets\n",
    "    def batch_generator(self):\n",
    "        i = 0\n",
    "        while True:\n",
    "            batch = self.triplets[i:i+self.batch_size//3].ravel()\n",
    "            \n",
    "            i += self.batch_size//3\n",
    "            if len(batch) == 0:\n",
    "                yield np.zeros((0, img_size, img_size, 3))\n",
    "            else:\n",
    "                result = pool.map(preprocess_image_worker_aug, batch)\n",
    "                X_batch = np.concatenate(result, axis=0)\n",
    "                yield X_batch, np.zeros(len(batch))\n",
    "\n",
    "    # return data generator for triplets\n",
    "    def get_generator(self):\n",
    "        gen = self.batch_generator()\n",
    "        return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pool of processes for parallel data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_triplets = len(train_user_images) # Need to check\n",
    "pool = mp.Pool(processes=8)\n",
    "sampler = SemiHardNegativeSampler(pool, batch_size, num_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach tensorboard to monitor learining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                 histogram_freq=1, \n",
    "                 write_graph=False, \n",
    "                 write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "Each epoch we \n",
    "- train model on triplets with semi-hard negatives from `sampler`\n",
    "- resample triplets\n",
    "- do validation and save model with frequency `valid_frequency`\n",
    "\n",
    "I use annoy index for nearest neighbors search to speedup validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tianxiangliu/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:502: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "50/50 - 626s - loss: 0.1842\n",
      "Epoch 2/2\n",
      "50/50 - 674s - loss: 0.1891\n",
      "Epoch 3/3\n",
      "50/50 - 835s - loss: 0.1758\n",
      "Epoch 4/4\n",
      "50/50 - 827s - loss: 0.1931\n",
      "Epoch 5/5\n",
      "50/50 - 844s - loss: 0.1822\n",
      "Epoch 6/6\n",
      "50/50 - 811s - loss: 0.1882\n",
      "Epoch 7/7\n",
      "50/50 - 841s - loss: 0.1901\n",
      "Epoch 8/8\n"
     ]
    }
   ],
   "source": [
    "# https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/\n",
    "for i in range(num_epoch):\n",
    "    train_gen = sampler.get_generator()\n",
    "    h = model.fit_generator(train_gen, \n",
    "                        steps_per_epoch=num_triplets//(batch_size//3), \n",
    "                        epochs=epoch+1, \n",
    "                        initial_epoch=epoch, \n",
    "                        verbose=2,\n",
    "                        # max_q_size=1, \n",
    "                        callbacks=[tensorboard,])\n",
    "    sampler.resample()\n",
    "    \n",
    "    if epoch%valid_frequency == 0 and epoch != 0:\n",
    "        seller_pred = model.predict_generator(batch_generator_predict(pool, 32, seller_images), \n",
    "                                              math.ceil(len(seller_images)/32), \n",
    "                                              # max_q_size=1, \n",
    "                                              workers=1)\n",
    "        val_user_pred = model.predict_generator(batch_generator_predict(pool, 32, val_user_images), \n",
    "                                                math.ceil(len(val_user_images)/32), \n",
    "                                                # max_q_size=1, \n",
    "                                                workers=1)\n",
    "        # val_user_clean_pred = model.predict_generator(batch_generator_predict(pool, 32, val_user_clean_images), math.ceil(len(val_user_clean_images)/32), max_q_size=1, workers=1)\n",
    "\n",
    "        search_index = AnnoyIndex(128, metric='euclidean')\n",
    "        for i in range(len(seller_pred)):\n",
    "            search_index.add_item(i, seller_pred[i])\n",
    "        search_index.build(50)\n",
    "\n",
    "        recall_scores = {i: [] for i in recall_values}\n",
    "        for i in range(len(val_user_pred)):\n",
    "            r = search_index.get_nns_by_vector(val_user_pred[i], 100)\n",
    "            val_cl = val_image_to_class[val_user_images[i]]\n",
    "            for k in recall_values:\n",
    "                recall_scores[k].append(recall(val_images[val_cl][0], [seller_images[i] for i in r[:k]]))\n",
    "\n",
    "        print ('val on full')\n",
    "        for k in recall_values:\n",
    "            print (k, np.mean(recall_scores[k]))\n",
    "\n",
    "        val_recall = [(k, np.mean(recall_scores[k])) for k in recall_values]\n",
    "\n",
    "        recall_scores = {i: [] for i in recall_values}\n",
    "\n",
    "#         for i in range(len(val_user_clean_pred)):\n",
    "#             r = search_index.get_nns_by_vector(val_user_clean_pred[i], 100)\n",
    "#             val_cl = val_image_clean_to_class[val_user_clean_images[i]]\n",
    "#             for k in recall_values:\n",
    "#                 recall_scores[k].append(recall(val_images_clean[val_cl][0], [seller_images[i] for i in r[:k]]))\n",
    "\n",
    "#         print ('val on clean')\n",
    "#         for k in recall_values:\n",
    "#             print (k, np.mean(recall_scores[k]))\n",
    "\n",
    "#         val_recall_clean = [(k, np.mean(recall_scores[k])) for k in recall_values]\n",
    "        \n",
    "        try:\n",
    "            with open(recall_log_file, 'r') as f:\n",
    "                recall_log = json.load(f)\n",
    "        except:\n",
    "            recall_log = []\n",
    "\n",
    "        recall_log.append((epoch, val_recall, val_recall_clean))\n",
    "\n",
    "        with open(recall_log_file, 'w') as f:\n",
    "            json.dump(recall_log, f)\n",
    "            \n",
    "        model.save_weights(os.path.join(checkpoint_dir, 'triplet_semihard_%d.keras'%epoch))\n",
    "        \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/triplet_semihard_v3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
